#!/usr/bin/env bash

RUNSFROM="$(cd "$(dirname "$0")" && pwd)"

. ./prod 

if [[ -z "${AWS_ACCESS_KEY_ID}" ]]; then
    echo "Missing AWS_ACCESS_KEY_ID"
    exit 99
fi

if [[ -z "${AWS_SECRET_ACCESS_KEY}" ]]; then
    echo "Missing AWS_SECRET_ACCESS_KEY"
    exit 99
fi

if [[ -z "${AWS_SESSION_TOKEN}" ]]; then
    echo "Missing AWS_SESSION_TOKEN"
    exit 99
fi

for BUCKET in s3://ovation-bioinformatics-pipeline-compute-prod-final-artifacts s3://ovation-pipeline-worker-prod-results; do
    echo "Processing VCF file data in ${BUCKET}"
    for KEY in $(aws s3 ls --recursive ${BUCKET} | grep '.unannotated.vcf.gz$' | awk '{print $4}'); do
        FILENAME=$(echo ${KEY} | rev | cut -d/ -f1 | rev)
        SEQUENCE=$(echo ${KEY} | cut -d/ -f1)
        SAMPLE=$(echo ${FILENAME} | cut -d. -f1)
        DATAFILE="files/${SEQUENCE}.vcf.gz"

        if [[ ! -f ${DATAFILE} ]]; then
            echo "Copying ${KEY} to ${DATAFILE}"
            aws s3 cp --quiet ${BUCKET}/${KEY} ${DATAFILE}
        fi

        if [[ ! -f vcf/${SAMPLE}.tsv ]]; then
            echo "Extracting VCF data for ${SAMPLE}"
            zcat ${DATAFILE} |
                awk -vSAMPLE=${SAMPLE} -vSEQUENCE=${SEQUENCE} -f ${RUNSFROM}/run-normals.awk | (
                sed -u 1q
                sort -t $'\t' -k2,2
            ) >vcf/${SAMPLE}.tsv
        fi

        # rm ${DATAFILE}
    done

    echo "Processing fastp file data in ${BUCKET}"
    for KEY in $(aws s3 ls --recursive ${BUCKET} | grep 'fastp.json$' | awk '{print $4}'); do
        FILENAME=$(echo ${KEY} | rev | cut -d/ -f1 | rev)
        SEQUENCE=$(echo ${KEY} | cut -d/ -f1)
        SAMPLE=$(echo ${FILENAME} | cut -d. -f1)
        DATAFILE="files/${SEQUENCE}.json"

        if [[ ! -f ${DATAFILE} ]]; then
            echo "Copying ${KEY} to ${DATAFILE}"
            aws s3 cp --quiet ${BUCKET}/${KEY} ${DATAFILE}
        fi

        if [[ ! -f reads/${SAMPLE}.tsv ]]; then
            echo "Extracting trimming data for ${SAMPLE}"

            READS_BEFORE=$(jq -r .summary.before_filtering.total_reads ${DATAFILE})
            BASES_BEFORE=$(jq -r .summary.before_filtering.total_bases ${DATAFILE})
            GC_BEFORE=$(jq -r .summary.before_filtering.gc_content ${DATAFILE})

            READS_AFTER=$(jq -r .summary.after_filtering.total_reads ${DATAFILE})
            BASES_AFTER=$(jq -r .summary.after_filtering.total_bases ${DATAFILE})
            GC_AFTER=$(jq -r .summary.after_filtering.gc_content ${DATAFILE})

            printf "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n" \
                $SEQUENCE \
                $SAMPLE \
                $READS_BEFORE $BASES_BEFORE $GC_BEFORE \
                $READS_AFTER $BASES_AFTER $GC_AFTER \
                >reads/${SAMPLE}.tsv
        fi

        # rm ${DATAFILE}
    done

    echo "Processing MultiQC file data in ${BUCKET}"
    for FILE in $(aws s3 ls --recursive ${BUCKET} | grep 'multiqc_data.json$' | awk '{print $1"/"$2"/"$4}'); do
        DATE=$(echo ${FILE} | cut -d/ -f1)
        TIME=$(echo ${FILE} | cut -d/ -f2)
        RUNID=$(echo ${FILE} | cut -d/ -f3)
        FILE=$(echo ${FILE} | cut -d/ -f3-)

        DATAFILE="mqc/${DATE}_${TIME}_${RUNID}.json"

        if [[ ! -f "${DATAFILE}" ]]; then
            aws s3 cp ${BUCKET}/${FILE} ${DATAFILE}
        fi
    done
done

# JSON trimming data
# write the header into the report and append the data from each trim report
echo "Building fastp / JSON trimming report"
printf "sequence\tsample\treads_before\tbases_before\tgc_before\treads_after\tbases_after\tgc_after\n" >trim-report.tsv
cat reads/*-fastp.tsv >>trim-report.tsv

# VCF variant data
# write the header into the report and append the data from each vcf report
echo "Building variant report"
grep --no-filename '^sequence' vcf/* | sort | uniq >vcf-report.tsv
grep --no-filename -v '^sequence' vcf/* >>vcf-report.tsv

# MultiQC results
# write the header into the report
echo "Building combined MultiQC report"
(
    printf "sequence\tsample\ttimestamp\t"
    printf "location\t"
    printf "biobambam2-PERCENT_DUPLICATION\t"
    printf "samtools-flagstat_total\tsamtools-mapped_passed\t"
    printf "picard-PCT_PF_READS_ALIGNED\tpicard-MEDIAN_COVERAGE\tpicard-MEAN_COVERAGE\tpicard-SD_COVERAGE\tpicard-PCT_30X\t"
    printf "samtools-error_rate\tsamtools-non_primary_alignments\tsamtools-reads_mapped\tsamtools-reads_mapped_percent\tsamtools-reads_properly_paired_percent\tsamtools-reads_MQ0_percent\tsamtools-raw_total_sequences"
    printf "\n"
) >mqc-report.tsv

# then walk over the buckets, again, and run the mqc extraction
# this would be a _lot_ faster if we could just pass the directory to node
for BUCKET in s3://ovation-bioinformatics-pipeline-compute-prod-final-artifacts s3://ovation-pipeline-worker-prod-results; do
    for MQC in mqc/*; do
        node ${RUNSFROM}/run-normals.js ${BUCKET} ${MQC} >>mqc-report.tsv
    done
done
